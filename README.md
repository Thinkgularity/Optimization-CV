Sem №1

В данном проекте реализована нейронная сеть для классификации изображений, в частности, для датасета CIFAR-10.

Структура проекта:

__pycache__: Папка, в которой Python хранит скомпилированные байт-коды и оптимизированные версии файлов. Эта папка создается автоматически при запуске модулей.

cifar_model.pth: Файл содержит веса предобученной модели, сохраненные в формате PyTorch. Этот файл используется для загрузки и инференса модели.

model.py: Этот файл содержит определение архитектуры нейронной сети, которую можно обучить для выполнения задач классификации изображений.

test.py:  Здесь мы тестируем модель для оценки ее точности на валидационном или тестовом наборе данных.

test_onnx: Этот файл используется для тестирования модели, экспортированной в формат ONNX (Open Neural Network Exchange), что позволяет использовать модель в различных фреймворках.

train.py: Этот файл предназначен для обучения нейронной сети, включая этапы загрузки данных, оптимизации и сохранения модели.


Sem № 2:

В данном проекте проводилось сравнение скорости сходимости модели при использовании различных алгоритмов оптимизации: **SGD (Stochastic Gradient Descent)** и **Adam**. 

## Результаты

На графике представлено изменение значения потерь (loss) по эпохам для обоих алгоритмов. 

![image](https://github.com/user-attachments/assets/5aca40f4-9ad0-4cef-9faa-470f949a93b5)


### Выводы

1. **SGD**: Сходимость модели была более волнообразной, однако в целом значение потерь снижалось.
   
2. **Adam**: Этот алгоритм демонстрировал более стремительное и стабильное снижение потерь, что говорит о более эффективной сходимости.

Выводя итог, можно сказать, что алгоритм **Adam** в большинстве случаев является предпочтительным выбором для обучения моделей, так как он обеспечивает более быструю сходимость по сравнению с **SGD**. Важно учесть, что выбор оптимизатора также может зависеть от конкретной задачи и свойств данных.


Sem № 3

# Оптимизация гиперпараметров с помощью Optuna

В данной работе проводилась настройка гиперпараметров сверточной нейронной сети (CNN), используя Optuna для оптимизации на основе датасета CIFAR-10.

## Описание эксперимента

### Модель
Вместо многослойного перцептрона, была построена сверточная нейронная сеть (CNN) с использованием слоев `Conv2D`, `ReLU`, `MaxPool2D`, `Flatten`, а также полносвязных слоев. При построении модели оптимизировались два гиперпараметра:

- **`n_layers`**: Число сверточных слоев (от 1 до 5).
- **`kernel_size`**: Размер ядра свертки (от 3 до 7).

### Алгоритм оптимизации
Для оптимизации использовались три различных алгоритма: Adam, RMSprop и SGD. Кроме того, был настроен переменный коэффициент обучения (`lr`), который также варьировался в диапазоне от 1e-5 до 1e-1.

### Результаты

После завершения 100 проб, наилучший набор гиперпараметров был определен, отражая лучшие значения точности.

![result](https://github.com/user-attachments/assets/c0b8c629-5ed9-4f3d-92d8-d58b47a5e345)

```
Best trial: Value: [значение точности] Params: n_layers: [значение] kernel_size: [значение] optimizer: [значение] lr: [значение]

```


Sem № 4 
```
console output example:

Torch Model Accuracy: 45.79%, Mean Inference Time: 0.84 ms
ONNX Model Accuracy: 45.79%, Mean Inference Time: 0.12 ms
```

ONNX использует onnxruntime, который оптимизирует граф вычислений и использует более эффективные механизмы выполнения. Он может автоматически применять различные оптимизации, такие как фуззинг операций и упрощение вычислений. ONNX может более эффективно управлять вычислениями, минимизируя накладные расходы, связанные с планированием и инициализацией в режиме выполнения.

В PyTorch некоторые функции, как например автоматическое дифференцирование и динамическое построение вычислительных графов, могут добавлять накладные расходы на этапе выполнения. ONNX, будучи статической моделью, не требует этих дополнительных накладных расходов.
